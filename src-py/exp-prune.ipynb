{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Pruning experiment to demonstrate memory savings from pruning feedforward weights.\n",
    "Doesn't use actual weights; simply for demonstration. See prune.py for script to prune Llama FF layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "from model import ModelArgs\n",
    "from convert import serialize_fp32\n",
    "from prune import serialize_bitvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Serializes a pruned Nx1 weight as follows:\n",
    "- first byte is an unsigned int max containing the number of elements:\n",
    "- next log(N) bytes is a bitvector containing a 1 if that number in the tensor is nonzero and 0 otherwise\n",
    "- padding to make the number of bytes a multiple of 8\n",
    "- every single nonzero value, saved as fp32\n",
    "\"\"\"\n",
    "def serialize_pruned_tensor(tensor, filename, bitvec=False):\n",
    "    num_elements = tensor.numel()\n",
    "    print(\"num elements\", num_elements)\n",
    "    bitvector = tensor.flatten().bool().view(-1).cpu().numpy().tobytes()\n",
    "    if bitvec:\n",
    "        bin_file = open(\"bin_\" + filename, \"wb+\")\n",
    "        serialize_bitvec(bin_file, tensor.flatten().bool())\n",
    "\n",
    "    num_nonzero = tensor.flatten().bool().sum().item()\n",
    "\n",
    "    with open(filename, \"wb+\") as f:\n",
    "        f.write(struct.pack(\"I\", num_elements))\n",
    "        f.write(bitvector)\n",
    "        padding = b\"\\x00\" * ((8 - len(bitvector)) % 8)\n",
    "        f.write(padding)\n",
    "        print(\"writing\", num_nonzero, \"nonzero values\")\n",
    "        serialize_fp32(f, tensor.flatten()[tensor.flatten().bool()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set ModelArgs\\\n",
    "config = ModelArgs()\n",
    "\n",
    "config.dim = 4096\n",
    "config.n_layers = 32\n",
    "config.n_heads = 32\n",
    "config.hidden_dim = 11008\n",
    "\n",
    "config.vocab_size = 32000\n",
    "config.max_seq_len = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same dims as model.layers[i].feed_forward.w1 (or w2 or w3)\n",
    "ff1 = nn.Linear(config.hidden_dim, config.dim)\n",
    "w = torch.randn(config.hidden_dim, config.dim)\n",
    "ff1.weight.data = w\n",
    "# commented out to avoid accidents, but if u want to test, uncomment\n",
    "# serialize_pruned_tensor(ff1.weight.data, 'full.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "full.bin should be 225.4 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone layer for later\n",
    "ff1_clone = nn.Linear(config.hidden_dim, config.dim)\n",
    "ff1_clone.weight = nn.Parameter(w.clone())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "llama with 20% weights pruned only exhibits 73%->69% performance loss  \n",
    "src: https://arxiv.org/pdf/2305.11627.pdfprune_amount = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune.l1_unstructured(ff1, name='weight', amount=0.2)\n",
    "# serialize_pruned_tensor(ff1.weight.data, 'pruned-02.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pruned-02.bin should be 189.4 MB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extremely aggressive pruning\n",
    "prune.l1_unstructured(ff1_clone, name='weight', amount=0.5)\n",
    "# remove orig weights\n",
    "prune.remove(ff1_clone, 'weight')\n",
    "# serialize_pruned_tensor(ff1_clone.weight.data, 'pruned-05.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pruned-05.bin should be 135.3 MB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv-kernel",
   "language": "python",
   "name": "local-venv-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
